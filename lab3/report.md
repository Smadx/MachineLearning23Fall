# lab3:Density Peak Clustering
## 1.实验目的
使用密度峰值聚类算法对数据集进行聚类
## 2.实验原理
### 1.密度峰值聚类算法
1. 局部密度$\rho_i$定义为样本$x_i$的距离小于$d_c$的样本个数. 
2. $\delta_i$定义为样本$x_i$到密度比其大的样本的最小距离. 
3. 密度峰定义为$\gamma = \rho \cdot \delta$较大的样本.
4. 根据密度峰的定义,可以得到聚类中心的个数,即密度峰的个数.
5. 把其它样本分配到密度峰上,即可得到聚类结果.
## 3.实验过程
### 1.数据预处理
使用``pandas``从``dataset``中读取数据,然后检查是否有缺失值.
### 2.画出数据分布图
使用``matplotlib``画出数据分布图,如下图所示:  
![](aggregation1.png)  
![](D311.png)  
![](R151.png) 
### 3.DPC算法
``class DPC``接受三个参数: ``Data``为数据集,``percent``用于计算截断距离,``sigma``为高斯核的参数. 
初始化时,首先计算各个样本点的距离矩阵,接着按照``percent``参数计算截断距离(即样本点的个数,占总样本点个数的百分比). 
然后按照高斯核函数计算样本点的局部密度,根据局部密度和距离矩阵计算样本点的$\delta$值. 
最后通过计算$\gamma$值,得到聚类中心的个数,并把其它样本分配到聚类中心上,得到聚类结果.    
``plot_decision_graph``函数用于画出决策图. 
### 4.聚类结果可视化与评估
首先画三个数据集的决策图,如下图所示:  
![](dg_1.png)  
![](dg_2.png)  
![](dg_3.png)  
根据决策图,可以确定聚类中心的个数分别为$7,31,15$. 
然后画出聚类结果,如下图所示:   
![](cl_1.png)  
![](cl_2.png)  
![](cl_3.png)  
最后使用``sklearn.metrics.davies_bouldin_score``函数计算DBI指数,得到的结果分别为$0.5400317701696621,0.5473027270482554,0.3153116182896666$.  
### 5.与sklearn中的其它聚类算法进行比较
我们选择DPC算法表现较差的数据集$Aggregation$来做测试,下面是结果: 
|Algorithm in sklearn|DBI|
|:-:|:-:|
|K-means|0.7331700637087574|
|Agglomerative|0.7100972513852566|
|DBSCAN|0.638602045454667|
|Mixtrue of Gaussian|0.7165892624151932| 

可以看到,DPC算法的DBI指数($0.5400317701696621$)最低,说明DPC算法的聚类效果最好. 
## 5.结果分析
DPC算法综合了K-means和DBSCAN的优点,在不同类型的数据集上都有较好的聚类效果. 
在调参的过程中,我发现$percent$参数对聚类结果有很大影响,如果$percent$参数设置的不合理,会出现把大聚类中心分成多个小聚类中心的情况. 另外,使用高斯核函数对模型的性能有很大帮助,如果不使用高斯核函数,聚类效果会很差(也可能是我没有调好参数). 
## 6.实验总结
本次实验我通过阅读论文学习了DPC聚类算法,锻炼了我阅读论文的能力.在完成本次实验的过程中,我从头开始实现了整个机器学习的框架,包括数据预处理,模型训练,模型评估等,这对我以后的学习有很大帮助.另外,在展示结果时,我学习了如何使用``matplotlib``画图,这充分锻炼了我的数据可视化能力,并且让我对``matplotlib``有了更深的理解.最后,我学习了如何使用``sklearn``中的聚类算法,并且学会了如何使用``sklearn.metrics.davies_bouldin_score``函数计算DBI指数,比较了不同聚类算法的性能差异.