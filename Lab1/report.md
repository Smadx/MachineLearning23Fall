# lab1:LogisticRegression
## 实验目的
本实验旨在探究逻辑回归模型在特定数据集上的表现，以评估其在二分类问题中的准确性和可靠性。通过分析模型在训练集和测试集上的性能表现，我们旨在确定模型的预测能力以及其在新数据上的泛化能力.
## 实验原理
### 1.Logistic回归
拟合$y=\frac{1}{1+e^{-\mathbf{w}^T\mathbf{x} +b} }$函数
### 2.损失函数
取负对数似然 
$$\sum_{i=1}^m(-y_i\mathbf{\beta}^T\mathbf{x}_i+\ln (1+e^{\mathbf{\beta}^T\mathbf{x}_i}))$$
### 3.参数优化
利用梯度下降法,其中 
$$grad=-\sum_{i=1}^m\mathbf{x}_i(y_i-p_1(\mathbf{x}_i;\mathbf{\beta}))$$
### 4.模型评估
以准确率acc作为标准 
$$acc=\frac{预测正确次数}{样本总数}$$
## 实验过程
### 1.数据预处理
依次完成如下步骤: 
1. 读入数据集
2. 检查缺失值并用均值填充
3. 把字符类型的数据转换为浮点型
4. 进行特征编码(pd.get_dummies)
5. 归一化(x=(x-min)/(max-min))(然后删除最后一列)
6. 划分特征和标签(最后一列)
7. 按4:1的比例划分训练集和测试集
这里我没有做类别不平衡处理,因为我尝试过欠采样数据,但是训练出的效果并不好. 
### 2.LogisticRegression算法
#### 1.fit
我把``max_iter``的默认参数改为了10000,因为输入1e7时报错了. 
计算``loss``的梯度(包含正则项). 
利用梯度下降法更新参数,最后计算当前epoch的损失
#### 2.predict
根据``liner_output``的输出,如果>=0.5则把``prob``置1,反之置0
### 3.Train(seed=3)
我在训练时取得较好结果的参数如下: 
```python
model=LogisticRegression('l2',0.001,True)

Losses=model.fit(X_train.values,y_train.values,0.0001,1e-7,2000)
```
然后输出Loss曲线 
![Alt text](image.png)
### 4.Test(seed=3)
根据训练结果,计算在训练集和测试集上的误差 
![](Test3.png)
### 5.参数比较
下面有些结果看上去比上面更好,这是因为有一定的随机性,而且下面的测试都有不稳定性,只是定性地展示一下结果. 
#### 1.正则化
对于``l1``和``l2``范数,分别对正则化系数$0.01\sim 0.05$进行训练和比较 
对于``l1``范数: 
![](accr1.png) 
![Alt text](image-1.png)
对于``l2``范数: 
![](accr2.png) 
![Alt text](image-2.png) 
可以看到,正则化显著降低了模型的学习能力,提高了模型的训练速度和泛化能力. 
#### 2.bias
根据我的调试,bias对结果影响不是很大,这里略过. 
#### 3.学习率和步数
我们将步长设定为10000步,比较不同学习率($0.1\sim 0.0001$)的训练情况. 
![](acclr.png) 
![Alt text](image-3.png) 
这里不画前两个是因为波动较大,严重干扰画面
## 实验结果分析
1. 欠采样效果不好可能是因为数据集较小
2. 梯度下降收敛较慢,使用牛顿法会更快收敛
3. 经过调试的model正确率大概稳定在80%多,感觉到上限了
4. 正则化表现并不突出,这可能是因为过拟合风险不高,但是正则化有效地加快了model训练速度
5. 学习率小于0.001的数量级时大概能得到较好的结果,再大就会有波动
## 总结
通过本次逻辑回归实验，我深入了解了逻辑回归模型在二分类问题中的应用。我观察到随着正则化参数的增加，模型在训练集上的准确率逐渐下降，但在测试集上的泛化能力有所提高。这表明适当的正则化可以有效避免模型过度拟合。此外，我的实验结果显示逻辑回归模型在处理二分类问题上具有可行性，并且能够对新数据进行较为准确的预测。然而，我们也意识到在处理复杂数据集和非线性关系时，逻辑回归模型可能表现不佳。因此，未来的工作可以探索其他更复杂的模型或采用特征工程的方法来提高模型的性能。总的来说，本次实验为我提供了一个深入了解逻辑回归模型的机会，并为进一步研究和应用机器学习算法奠定了基础.