# lab2:SVM
## 实验目的
使用两种不同的方法在线性可分数据集上实现SVM算法,并进行比较
## 实验原理
### 1.SMO算法
SMO算法是一种启发式算法,每次循环选择两个变量进行优化,其他变量保持不变.
步骤如下:
1. 选择第一个变量,即违反KKT条件最严重的点
2. 选择第二个变量,使得目标函数有足够大的变化
3. 优化这两个变量,首先计算边界,然后根据约束条件计算第二个变量的取值,最后计算第一个变量的取值
4. 如果两个变量都没有足够大的变化,则重新选择第一个变量
5. 如果所有变量都没有足够大的变化,则退出循环
6. 重复1-5
7. 计算b
8. 计算w
### 2.梯度下降法
SVM问题的损失函数为
$$L=\sum_{i=1}^ml_{hinge}(y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)-1)+\gamma \|\boldsymbol{w}\|^2$$
有,
$$
\begin{align*}
    \frac{\partial L}{\partial \boldsymbol{w}}&=-\sum_{i=1}^my_i\boldsymbol{x}_i\mathbb{I}(y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)<1)+2\gamma \boldsymbol{w}\\
    \frac{\partial L}{\partial b}&=-\sum_{i=1}^my_i\mathbb{I}(y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)<1)
\end{align*}
$$
其中$\mathbb{I}$为指示函数,当$y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)<1$时为1,否则为0.
## 实验过程
### 1.数据预处理
按照助教提供的``generate_data``函数生成数据集,20维,10000个,然后按照4:1的比例划分训练集和测试集
### 2.SMO算法
首先``__init__``函数初始化参数,然后``fit``函数进行训练,``p``函数对单个样本进行预测,``predict``函数对整个测试集进行预测,``select_second_variable``函数选择第二个变量,``compute_L_H``函数计算边界,在最大更新步数内循环,每次选择两个变量进行优化,最后计算b和w
### 3.梯度下降法
首先``__init__``函数初始化参数,然后``fit``函数进行训练,``p``函数对单个样本进行预测,``predict``函数对整个测试集进行预测,``grad``函数计算梯度,为了提高计算效率我把每16个样本作为一个batch,进行epoch次迭代,最后计算b和w,我使用了hinge loss,并且加入了L2正则化,在正则化系数为0.1是效果较好.
### 4.训练
上述两种方法的训练时间分别为120,233(epoch=10),另外,我使用``skearn``的``SVC``函数进行训练,训练时间为19s,这说明我的代码还有很大的优化空间.
## 测试与结果分析
测试结果如下(分别为SMO,梯度下降,``skearn``): 
![](结果.png) 
事实上,仅训练一个epoch的梯度下降法的acc就可以达到0.84左右,用时只需23s,这说明梯度下降法的效率比SMO高很多,这可能是因为我使用了batch,而且我没有对SMO进行优化.在未使用batch的情况下,梯度下降法运行一个epoch的时间为340s左右,这说明batch的使用对效率有很大的提升.不过我的两种方法都没有达到``skearn``的效果,这可能是因为我没有对SMO进行优化,而且我使用了hinge loss,而``skearn``使用的是``libsvm``.
## 实验总结
通过本次实验，我学到了支持向量机的基本原理和算法。这包括线性 SVM 的目标函数、优化问题以及使用梯度下降和SMO算法来求解这个优化问题。我成功地用 Python 和 NumPy 实现了一个简化的线性 SVM 模型。这需要对 SVM 的数学背后的原理有深入的理解，以及对 Python 编程和 NumPy 库的熟练应用。在实验过程中，我尝试了不同的学习率、正则化强度等超参数，以找到最佳的模型性能。这为我提供了调参的实践经验，帮助我更好地理解模型对不同参数的敏感性。通过实现梯度下降算法，并将其扩展到使用批处理（mini-batch）来进行训练，我对优化算法的理解和应用得到了加强。